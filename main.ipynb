{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas, Numpy and Matplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploratory Data Analysis (EDA) of Cash Request \n",
    "\n",
    "\"\"\"Before diving into cohort analysis, conduct an exploratory data analysis to gain a \n",
    "comprehensive understanding of the dataset. Explore key statistics, distributions, \n",
    "and visualizations to identify patterns and outliers. EDA will help you make informed \n",
    "decisions on data preprocessing and analysis strategies.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Read the document and Create a Dataframe\n",
    "# Document type: CSV\n",
    "df_cash = pd.read_csv(\"project_dataset\\extract - cash request - data analyst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Start the Exploratory Analysis\n",
    "\n",
    "# a: Shape of the DataFrame (rows, columns)\n",
    "data_shape = df_cash.shape\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(data_shape)\n",
    "\n",
    "# b: Column Names\n",
    "column_names = df_cash.columns\n",
    "print(\"Column Names:\")\n",
    "print(column_names)\n",
    "\n",
    "# c: Data types of each column\n",
    "data_types = df_cash.dtypes\n",
    "print(\"Data Types:\")\n",
    "print(data_types)\n",
    "\n",
    "# d: General Information about the DataFrame\n",
    "data_info = df_cash.info()\n",
    "print(\"General Information about the DataFrame:\")\n",
    "print(data_info)\n",
    "\n",
    "# e: Statistical Summary\n",
    "data_summary = df_cash.describe()\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Quality Analysis of Cash Request\n",
    "\n",
    "\"\"\"Assess the quality of the dataset by identifying missing values, \n",
    "data inconsistencies, and potential errors. \n",
    "Implement data cleaning and preprocessing steps to ensure the reliability of your analysis.\n",
    "Document any data quality issues encountered and the steps taken to address them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Start the Data Quality Analysis\n",
    "\n",
    "# a: Check for missing values\n",
    "# Check for missing values in the DataFrame and clean the dataFrame\n",
    "missing_values = pd.isnull(df_cash)\n",
    "df_cash_cleaned = df_cash.drop_duplicates().fillna(0)\n",
    "# Count missing values in each column\n",
    "missing_counts = missing_values.sum()\n",
    "\n",
    "# Count columns with missing values\n",
    "columns_with_missing = missing_counts[missing_counts > 0].count()\n",
    "\n",
    "# Check if all columns have missing values\n",
    "all_columns_missing = missing_counts.all()\n",
    "\n",
    "# Calculate the total number of missing values\n",
    "total_missing_values = missing_counts.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Values in Each Column:\\n\", missing_counts)\n",
    "print(\"\\nNumber of Columns with Missing Values:\", columns_with_missing)\n",
    "print(\"All Columns Have Missing Values:\", all_columns_missing)\n",
    "print(\"\\nTotal Missing Values in the DataFrame:\", total_missing_values)\n",
    "print(df_cash_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploratory Data Analysis (EDA) of Fees\n",
    "\n",
    "\"\"\"Before diving into cohort analysis, conduct an exploratory data analysis to gain a \n",
    "comprehensive understanding of the dataset. Explore key statistics, distributions, \n",
    "and visualizations to identify patterns and outliers. EDA will help you make informed \n",
    "decisions on data preprocessing and analysis strategies.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Read the document and Create a Dataframe\n",
    "# Document type: CSV\n",
    "df_fees = pd.read_csv(\"project_dataset/extract - fees - data analyst - .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Start the Exploratory Analysis\n",
    "\n",
    "# a: Shape of the DataFrame (rows, columns)\n",
    "data_shape = df_fees.shape\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(data_shape)\n",
    "\n",
    "# b: Column Names\n",
    "column_names = df_fees.columns\n",
    "print(\"Column Names:\")\n",
    "print(column_names)\n",
    "\n",
    "# c: Data types of each column\n",
    "data_types = df_fees.dtypes\n",
    "print(\"Data Types:\")\n",
    "print(data_types)\n",
    "\n",
    "# d: General Information about the DataFrame\n",
    "data_info = df_fees.info()\n",
    "print(\"General Information about the DataFrame:\")\n",
    "print(data_info)\n",
    "\n",
    "# e: Statistical Summary\n",
    "data_summary = df_fees.describe()\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data_summary)\n",
    "print(df_fees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Quality Analysis of Fees\n",
    "\n",
    "\"\"\"Assess the quality of the dataset by identifying missing values, \n",
    "data inconsistencies, and potential errors. \n",
    "Implement data cleaning and preprocessing steps to ensure the reliability of your analysis.\n",
    "Document any data quality issues encountered and the steps taken to address them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Start the Data Quality Analysis\n",
    "\n",
    "# a: Check for missing values\n",
    "# Check for missing values in the DataFrame\n",
    "missing_values = pd.isnull(df_fees)\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_counts = missing_values.sum()\n",
    "df_fees_cleaned = df_fees.drop_duplicates().fillna(0)\n",
    "\n",
    "# Count columns with missing values\n",
    "columns_with_missing = missing_counts[missing_counts > 0].count()\n",
    "\n",
    "# Check if all columns have missing values\n",
    "all_columns_missing = missing_counts.all()\n",
    "\n",
    "# Calculate the total number of missing values\n",
    "total_missing_values = missing_counts.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Values in Each Column:\\n\", missing_counts)\n",
    "print(\"\\nNumber of Columns with Missing Values:\", columns_with_missing)\n",
    "print(\"All Columns Have Missing Values:\", all_columns_missing)\n",
    "print(\"\\nTotal Missing Values in the DataFrame:\", total_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploratory Data Analysis (EDA) of Lexique \n",
    "\n",
    "\"\"\"Before diving into cohort analysis, conduct an exploratory data analysis to gain a \n",
    "comprehensive understanding of the dataset. Explore key statistics, distributions, \n",
    "and visualizations to identify patterns and outliers. EDA will help you make informed \n",
    "decisions on data preprocessing and analysis strategies.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Read the document and Create a Dataframe\n",
    "# Document type: XLSX\n",
    "df_lexique = pd.read_excel(\"project_dataset/Lexique - Data Analyst.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Start the Exploratory Analysis\n",
    "\n",
    "# a: Shape of the DataFrame (rows, columns)\n",
    "data_shape = df_lexique.shape\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(data_shape)\n",
    "\n",
    "# b: Column Names\n",
    "column_names = df_lexique.columns\n",
    "print(\"Column Names:\")\n",
    "print(column_names)\n",
    "\n",
    "# c: Data types of each column\n",
    "data_types = df_lexique.dtypes\n",
    "print(\"Data Types:\")\n",
    "print(data_types)\n",
    "\n",
    "# d: General Information about the DataFrame\n",
    "data_info = df_lexique.info()\n",
    "print(\"General Information about the DataFrame:\")\n",
    "print(data_info)\n",
    "\n",
    "# e: Statistical Summary\n",
    "data_summary = df_lexique.describe()\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Quality Analysis of Lexique\n",
    "\n",
    "\"\"\"Assess the quality of the dataset by identifying missing values, \n",
    "data inconsistencies, and potential errors. \n",
    "Implement data cleaning and preprocessing steps to ensure the reliability of your analysis.\n",
    "Document any data quality issues encountered and the steps taken to address them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Start the Data Quality Analysis\n",
    "\n",
    "# a: Check for missing values\n",
    "# Check for missing values in the DataFrame\n",
    "missing_values = pd.isnull(df_lexique)\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_counts = missing_values.sum()\n",
    "\n",
    "# Count columns with missing values\n",
    "columns_with_missing = missing_counts[missing_counts > 0].count()\n",
    "\n",
    "# Check if all columns have missing values\n",
    "all_columns_missing = missing_counts.all()\n",
    "\n",
    "# Calculate the total number of missing values\n",
    "total_missing_values = missing_counts.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing Values in Each Column:\\n\", missing_counts)\n",
    "print(\"\\nNumber of Columns with Missing Values:\", columns_with_missing)\n",
    "print(\"All Columns Have Missing Values:\", all_columns_missing)\n",
    "print(\"\\nTotal Missing Values in the DataFrame:\", total_missing_values)\n",
    "\n",
    "print(df_lexique)\n",
    "print(df_cash_cleaned['created_at'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics to Analyze\n",
    "\n",
    "\"\"\"You will calculate and analyze the following metrics for each cohort:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 1. \n",
    "\"\"\"**Frequency of Service Usage:** \n",
    "Understand how often users \n",
    "from each cohort utilize IronHack Payments' cash advance services over time.\n",
    "\n",
    "For this metric we are going to build the cohorts based on \"USER_ID\" and \"created_at\" \n",
    "\"\"\"\n",
    "\n",
    "# Number of unique users:\n",
    "target_columns = [\"user_id\", \"amount\"]\n",
    "columns_to_colapse = [\"user_id\"]\n",
    "aggregations = ['count', 'sum', 'mean', 'max', 'min']\n",
    "users = df_cash_cleaned[target_columns].groupby(by=columns_to_colapse).agg(aggregations)\n",
    "\n",
    "print(\"Total number of unique users:\")\n",
    "print(users.count().unique()) # 10798 unique users\n",
    "\n",
    "# Create the cohorts based on \"created_at\" and \"user_id\". Important to group time data by MONTH \"M\"\n",
    "df_cash_cleaned['created_at'] = pd.to_datetime(df_cash_cleaned['created_at'])\n",
    "cohorts = df_cash_cleaned.groupby(df_cash_cleaned['created_at'].dt.to_period(\"M\"))['amount'].count()\n",
    "\n",
    "# Create the visualization chart using \"plt\" to see the number of transactions per month, over time\n",
    "cohorts.plot.bar(figsize=(12, 6))\n",
    "plt.title('Service user over time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 2.\n",
    "\"\"\"**Incident Rate:** Determine the incident rate, specifically focusing on payment incidents, for each cohort. \n",
    "Identify if there are variations in incident rates among different cohorts.\"\"\"\n",
    "\n",
    "# Sort the dataframe by type.\n",
    "df_fees_sorted = df_fees_cleaned.sort_values(by='type', ascending=True)\n",
    "\n",
    "# Slice the DataFrame to keep only the rows which column 'type' = 'incident'\n",
    "df_fees_incidents = df_fees_cleaned[df_fees_cleaned['type'] == \"incident\"] # DataFrame with all the incidents\n",
    "\n",
    "# Calculate the number of transactions by count of 'df_fees_incidents'\n",
    "number_of_incidents = df_fees_incidents['type'].count() # Returns 2196 \n",
    "\n",
    "# Merge (inner) both \"Cash Request\" and \"Fees\" matching the 'cash_request_id' from 'df_fees_incidents with 'id' on 'df_cash'\n",
    "merged_df = pd.merge(df_fees_incidents, df_cash_cleaned, left_on=\"cash_request_id\", right_on='id', how='inner')\n",
    "\n",
    "# Group the incidents based on the month the request was created at and count them\n",
    "merged_df['created_at_y'] = pd.to_datetime(merged_df['created_at_y'])\n",
    "number_of_incidents_per_month = merged_df.groupby(merged_df['created_at_y'].dt.to_period(\"M\"))['type'].count()\n",
    "print(merged_df.head())\n",
    "\n",
    "# Display 'cohort', 'number_of_incidents_per_month\n",
    "print(cohorts.head())\n",
    "print(number_of_incidents_per_month.head())\n",
    "print(merged_df['created_at_y'].min())\n",
    "\n",
    "# Calculate the incident rate per month (percentage of number of incidents over total transactions in the same cohort)\n",
    "incident_rate = (number_of_incidents_per_month / cohorts)*100\n",
    "print(incident_rate)\n",
    "\n",
    "# Display the incidents per month on a chart\n",
    "incident_rate.plot.bar(figsize=(12, 6))\n",
    "plt.title('Incident Rate Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Percentage(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 3.\n",
    "\"\"\"**Revenue Generated by the Cohort:** Calculate the total revenue generated by \n",
    "each cohort over months to assess the financial impact of user behavior.\"\"\"\n",
    "\n",
    "# Filter the dataFrame 'df_fees' by 'status' = 'accepted'\n",
    "df_fees_accepted = df_fees_cleaned[df_fees_cleaned['status'] == \"accepted\"]\n",
    "merged_df = pd.merge(df_fees_accepted, df_cash_cleaned, left_on=\"cash_request_id\", right_on='id', how='inner')\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Groub 'total_amount' by month based on 'created_at'\n",
    "merged_df['created_at_y'] = pd.to_datetime(merged_df['created_at_y'])\n",
    "revenue_per_month = merged_df[['created_at_y', 'total_amount']].groupby(merged_df['created_at_y'].dt.to_period(\"M\"))['total_amount'].count()\n",
    "print(revenue_per_month)\n",
    "\n",
    "# Display the revenue per month on a chart\n",
    "revenue_per_month.plot.bar(figsize=(12, 6))\n",
    "plt.title('Revenue Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 4.\n",
    "\"\"\"**Loss of Revenue due to cancelled transaction**.\"\"\"\n",
    "\n",
    "# Filter the dataFrame 'df_fees' by 'status' = 'accepted'\n",
    "\n",
    "cancelled = df_fees_cleaned[df_fees_cleaned['status']=='cancelled'].reset_index()\n",
    "merged_cancelled_and_cash = pd.merge(cancelled, df_cash_cleaned, left_on=\"cash_request_id\", right_on='id', how='inner')\n",
    "\n",
    "# Groub 'total_amount' by month based on 'created_at_y'\n",
    "merged_cancelled_and_cash['created_at_y'] = pd.to_datetime(merged_cancelled_and_cash['created_at_y'])\n",
    "cancelled_cohorts = merged_cancelled_and_cash[['created_at_y', 'total_amount']].groupby(merged_cancelled_and_cash['created_at_y'].dt.to_period(\"M\"))['total_amount'].sum()\n",
    "print(cancelled_cohorts)\n",
    "\n",
    "# Display the loss of revenue per month on a chart\n",
    "cancelled_cohorts.plot.bar(figsize=(12, 6))\n",
    "plt.title('Loss Revenue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.show()\n",
    "\n",
    "cohorts.plot.bar(figsize=(12, 6))\n",
    "plt.title('Frequency of Service Usage over time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Transactions')\n",
    "plt.show()\n",
    "incident_rate.plot.bar(figsize=(12, 6))\n",
    "plt.title('Incident Rate Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Percentage(%)')\n",
    "plt.show()\n",
    "revenue_per_month.plot.bar(figsize=(12, 6))\n",
    "plt.title('Revenue Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.show()\n",
    "cancelled_cohorts.plot.bar(figsize=(12, 6))\n",
    "plt.title('Loss Revenue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
